{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# from collections import defaultdict\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "# import seaborn as sns\n",
    "\n",
    "from AttMem2Seq import AttMem2Seq\n",
    "from InformationCenter import InformationCenter\n",
    "# from InformationCenter_sample_train import InformationCenter\n",
    "from LoggingModule import LoggingModule\n",
    "\n",
    "#import gc #;gc.collect() # garbage collector \n",
    "import sys\n",
    "# raise NotImplementedError\n",
    "\n",
    "torch.manual_seed(12)\n",
    "torch.cuda.manual_seed(12)\n",
    "np.random.seed(12)\n",
    "random.seed(12)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name :  AttMem2Seq_v5.3_Ptr&Mem2AttGate\n",
      "\n",
      "voc_size : 19549\n",
      "num usr : 19675\n",
      "num_prd : 80256\n",
      "num_rating : 5\n",
      "size of train data : 655934\n",
      "size of dev   data : 93703\n",
      "size of test  data : 187396\n",
      "\n",
      "emb_size: 512\n",
      "hidden_size: 512\n",
      "latent_size: 64\n",
      "lstm_num_layers: 2\n",
      "dropout: 0.2\n",
      "max_gen_length: 70\n",
      "num_memory: 10\n",
      "\n",
      "batch size: 50\n",
      "maximum epoch: 100\n",
      "check step: 500\n",
      "evaluation step: 10000\n",
      "gradient clipping: 5\n",
      "initial learning rate: 0.002\n",
      "learning rate decaying: 0.97\n",
      "decay after: 10 epoch\n",
      "smoothing constant: 0.95\n",
      "\n",
      "word distribution stablizing epsilon: 1e-06\n",
      "\t\t\t\n",
      "************************** Additional Note **************************\n",
      "\t\t\t\n",
      "\t\t\tdropout only for lstm and meta embedding\n",
      "\n",
      "\t\t\tgradient clip : 3\n",
      "\n",
      "\t(AttMem2Seq.py) : no drop out for all parameters\n",
      "\t\n",
      "\tepsilon plus after pack prediction\n",
      "\t(you should change loss calculation in main.py and eval_nll from AttMem2Seq.py)\n",
      "*********************************************************************\n",
      "\t\t\t\n",
      "  copy model python file to debug folder \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MODEL DECLARATION\n",
    "IC = InformationCenter()\n",
    "model = AttMem2Seq(information_center=IC)\n",
    "if torch.cuda.is_available(): model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(IC.param_dir, IC.model_name+'.pth'))\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttMem2Seq(\n",
       "  (Encoder): Encoder(\n",
       "    (word_embed): Embedding(19549, 512, padding_idx=0)\n",
       "    (attribute_encoder): AttributeEncoder(\n",
       "      (user_emb): MetaEmbedding(\n",
       "        (meta_emb): Embedding(19675, 64)\n",
       "        (dropout): Dropout(p=0.2)\n",
       "      )\n",
       "      (product_emb): MetaEmbedding(\n",
       "        (meta_emb): Embedding(80256, 64)\n",
       "        (dropout): Dropout(p=0.2)\n",
       "      )\n",
       "      (rating_emb): MetaEmbedding(\n",
       "        (meta_emb): Embedding(5, 64)\n",
       "        (dropout): Dropout(p=0.2)\n",
       "      )\n",
       "      (H): Linear(in_features=192, out_features=2048, bias=True)\n",
       "      (W): Linear(in_features=576, out_features=1, bias=True)\n",
       "    )\n",
       "    (memory_encoder): MemoryEncoder(\n",
       "      (word_embed): Embedding(19549, 512, padding_idx=0)\n",
       "      (LSTM): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
       "      (W): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (z): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (Ws): Linear(in_features=1600, out_features=1, bias=True)\n",
       "  )\n",
       "  (Decoder): Decoder(\n",
       "    (LSTM): LSTM(\n",
       "      (rnn): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.2)\n",
       "    )\n",
       "    (word_predict): WordPredict(\n",
       "      (W): Linear(in_features=512, out_features=19549, bias=True)\n",
       "    )\n",
       "    (Wah): Linear(in_features=576, out_features=512, bias=True)\n",
       "    (dropout_for_attribute): Dropout(p=0.2)\n",
       "    (Wmh): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout_for_memory): Dropout(p=0.2)\n",
       "    (Wg): Linear(in_features=1600, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_ij(a):\n",
    "    pt = np.argmax(a)\n",
    "    i_ = int(pt/a.shape[1])\n",
    "    j_ = pt-i_*a.shape[1]\n",
    "    return i_, j_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, IC, id, user, product, rating, K=10):\n",
    "    # Validity Check\n",
    "    (_, _, user_in_data, product_in_data, _, _, _, _) = IC.split_data(IC.test_data[id:id+1], sorting=True, padding=True)\n",
    "    if user_in_data[0]!=user or product_in_data[0]!=product:\n",
    "        print(\"inconsistent id, user, product tuple\")\n",
    "        return\n",
    "    \n",
    "    # Language Generation\n",
    "    maxlen = IC.max_gen_length\n",
    "\n",
    "    # Extract Memory\n",
    "    memory_index_list = IC.memory_index_test_list\n",
    "    memory_score_list = IC.memory_score_test_list\n",
    "    memory_index = memory_index_list[id]\n",
    "    memory_score = memory_score_list[id]\n",
    "    argmax_index = np.argsort(-memory_score)\n",
    "    sampled_index = memory_index[argmax_index[:IC.num_memory]]\n",
    "    num_memory = len(sampled_index)\n",
    "    # memory word and length\n",
    "    w = IC.word_train[sampled_index].copy()\n",
    "    l = IC.length_train[sampled_index].copy()\n",
    "    memory_word = []\n",
    "    memory_length = []\n",
    "    for i in range(num_memory):\n",
    "        memory_word.append(w[i][1:-1]) # remove <sos> <eos>\n",
    "        memory_length.append(l[i]-2) # remove <sos> <eos>\n",
    "    memory_word, memory_length = np.array(memory_word), np.array(memory_length)\n",
    "    # sorting w.r.t. memory length\n",
    "    sorted_idx      = np.argsort(-memory_length)\n",
    "    memory_word \t= memory_word \t[sorted_idx]\n",
    "    memory_length   = memory_length [sorted_idx]\n",
    "    # padding \n",
    "    memory_word = np.array(IC.add_pad(memory_word, memory_length))\n",
    "    # variable wrapping\n",
    "    memory_word = IC.to_var(torch.from_numpy(memory_word.astype(np.int64)))\n",
    "    memory_for = [0]*num_memory\n",
    "    \n",
    "    user, product, rating = np.array([user]*K), np.array([product]*K), np.array([rating]*K)\n",
    "    \n",
    "    # Inital Hidden State with meta information\n",
    "    attributes = model.Encoder.attribute_encoder(user=user, product=product, rating=rating) # 1, 3, latent_size\n",
    "    decoder_hidden_init = model.Encoder.attribute_encoder.decoder_hidden_init(attributes=attributes)\n",
    "    # Generated Sentence Batch\n",
    "    sentence_batch = np.zeros((K, maxlen+1), dtype=np.int64) + IC.word2idx['<pad>']\n",
    "    sos_word_idx = np.zeros((K), dtype=np.int64)+int(IC.word2idx['<sos>'])\n",
    "    word_idx = sos_word_idx\n",
    "    sentence_batch[:, 0] = word_idx\n",
    "    likelihood_batch = np.zeros((K), dtype=np.float64) # likelihood: log-probability\n",
    "    for i in range(1, maxlen+1):\n",
    "        # LSTM next step\n",
    "        word_var = IC.to_var(torch.from_numpy(word_idx.astype(np.int64))).view(-1, 1)\n",
    "        word_emb = model.Encoder.word_embedding(word_var)\n",
    "        # print(\"word_emb: \",word_emb.size())\n",
    "        decoder_hidden, decoder_hidden_init = model.Decoder.LSTM(word_emb=word_emb, length=[1]*K, initial_hidden=decoder_hidden_init)\n",
    "        # print(\"decoder_hidden: \",decoder_hidden.size())\n",
    "        # print(\"decoder_hidden_init: \",decoder_hidden_init[0].size(),decoder_hidden_init[1].size())\n",
    "        # encoder attribute reduce using decoder hidden state\n",
    "        attribute_reduced, _ = model.Encoder.attribute_encoder.reduce_upr_with_attention(\n",
    "                user_emb=attributes[:, 0, :], \n",
    "                product_emb=attributes[:, 1, :],\n",
    "                rating_emb=attributes[:, 2, :],\n",
    "                decoder_hidden=decoder_hidden)\n",
    "        # print(\"attribute_reduced: \",attribute_reduced.size())\n",
    "        # decoder hidden with attribute\n",
    "        hidden_with_attribute = model.Decoder.hidden_with_attribute(\n",
    "            attribute_reduced=attribute_reduced, \n",
    "            context=decoder_hidden)\n",
    "        # print(\"hidden_with_attribute: \",hidden_with_attribute.size())\n",
    "        # explicit memory representation\n",
    "        memory_hidden = model.Encoder.memory_encoder(word=memory_word, length=memory_length) # 1*num_memory, memory length, hidden_size\n",
    "        # print(\"memory_hidden: \",memory_hidden.size())\n",
    "\n",
    "        # memory structuring: from (K*num_memory, memory_length, hidden_size) to (K, num_memory*memory_length, hidden_size)\n",
    "        l = memory_length\n",
    "        m = memory_hidden # num_memory, memory_length, hidden_size\n",
    "        # from (num_memory, memory_length, hidden_size) to (num_memory*memory_length, hidden_size)\n",
    "        memory_flat = m[0][:l[0]] # length, hidden_size\n",
    "        for j in range(1, num_memory):\n",
    "            m_ = m[j][:l[j]]\n",
    "            memory_flat = torch.cat([memory_flat, m_], dim=0)\n",
    "        # print(\"memory_flat: \",memory_flat.size(), l.sum())        \n",
    "        # memory_flat: length*num_memory, hidden_size\n",
    "        memory_batch = [memory_flat]*K\n",
    "        \n",
    "        # reducing memory using attention with hidden_with_attribute\n",
    "        memory_reduced, memory_attention = model.Encoder.memory_encoder.reduce_memory_with_attention(\n",
    "        memory_hidden=memory_batch, \n",
    "        query_batch=hidden_with_attribute,\n",
    "        length=[1]*K) # K, decoder_length, hidden_size\n",
    "        # print(\"memory_reduced: \",memory_reduced.size())\n",
    "        # decoder hidden with memory\n",
    "        hidden_with_memory = model.Decoder.hidden_with_memory(\n",
    "            memory_reduced=memory_reduced, \n",
    "            context=decoder_hidden)\n",
    "        # print(\"hidden_with_memory: \",hidden_with_memory.size())\n",
    "\n",
    "        # memory or attribute? explicit(specific) or implicit(general)?\n",
    "        memory_gate = model.Encoder.memory_or_attribute(\n",
    "            context=torch.cat([\n",
    "                attribute_reduced,\n",
    "                memory_reduced,\n",
    "                decoder_hidden,\n",
    "                word_emb,\n",
    "                ], dim=2)) # K, decoder_length, 1\n",
    "        # print(\"memory gate: \", memory_gate.size())\n",
    "        hidden_final = (memory_gate*hidden_with_memory) + (1-memory_gate)*hidden_with_attribute\n",
    "        # print(\"hidden_final: \",hidden_final.size())\n",
    "        # predicted word distribution\n",
    "        word_distribution = F.softmax(model.Decoder.word_predict(hidden_final), dim=2)\n",
    "        # print(\"word_distribution : \", word_distribution.size())\n",
    "        # copying mechanism\n",
    "        copy_gate = model.Decoder.copy_gate(\n",
    "            context=torch.cat([\n",
    "                attribute_reduced,\n",
    "                memory_reduced,\n",
    "                decoder_hidden,\n",
    "                word_emb,\n",
    "                ], dim=2)\n",
    "            )\n",
    "        # print(\"copy_gate: \", copy_gate.size())\n",
    "\n",
    "        word_distribution = (1-copy_gate) * word_distribution\n",
    "        memory_attention = copy_gate * memory_attention\n",
    "        # print(memory_attention.size())\n",
    "\n",
    "        \n",
    "        \n",
    "        l = memory_length\n",
    "        m = memory_word # num_memory, memory_length, hidden_size\n",
    "        # from (num_memory, memory_length, hidden_size) to (num_memory*memory_length, hidden_size)\n",
    "        memory_flat = m[0][:l[0]] # length, hidden_size\n",
    "        for j in range(1, num_memory):\n",
    "            m_ = m[j][:l[j]]\n",
    "            memory_flat = torch.cat([memory_flat, m_], dim=0)\n",
    "        # print(\"memory_flat: \",memory_flat.size(), l.sum())\n",
    "        # memory_flat: length*num_memory, hidden_size\n",
    "        memory_word_flatten = [memory_flat]*K\n",
    "        # print(\"memory_word_flatten: \",memory_word_flatten)\n",
    "        \n",
    "        \n",
    "        \n",
    "        memory_word_padded = model.Encoder.memory_encoder.memory_word_padding(\n",
    "            memory_word_flatten=memory_word_flatten,\n",
    "            ) # 1, num_memory*memory_length\n",
    "        memory_word_padded = memory_word_padded.unsqueeze(dim=1).repeat(1, memory_attention.size(1), 1)\n",
    "        # memory_word_flatten: K, num_memory*memory_length\n",
    "        # memory_attention: K, decoder_length, num_memory*memory_length\n",
    "        word_distribution = word_distribution.scatter_add_(2, memory_word_padded, memory_attention) # K, decoder_length, vocab_size\n",
    "\n",
    "        # print(\"word_distribution: \",word_distribution)\n",
    "        word_distribution = word_distribution.cpu().data.numpy()[:,0,:] # K, vocab_size\n",
    "        # print(\"word_distribution: \",word_distribution.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for ibest in range(K):\n",
    "            if sentence_batch[ibest, i-1]==IC.word2idx['<eos>']:\n",
    "                word_distribution[ibest] = likelihood_batch[ibest]\n",
    "            else:\n",
    "                word_distribution[ibest] = likelihood_batch[ibest]+np.log(word_distribution[ibest])\n",
    "            \n",
    "        # likelihood: log-probability\n",
    "        # word_distribution: log-probability , ndarray (K, vocab_size)\n",
    "        # choose k best - decoder_hidden_init, word_idx\n",
    "        h_init, c_init = decoder_hidden_init\n",
    "        h_init, c_init = h_init.cpu().data, c_init.cpu().data\n",
    "        best_h_init, best_c_init = torch.zeros(2, K, IC.hidden_size), torch.zeros(2, K, IC.hidden_size)\n",
    "        best_word_idx = np.zeros((K), dtype=np.int64)\n",
    "        best_sentence = np.zeros((K, maxlen+1), dtype=np.int64)\n",
    "        for ii in range(K):\n",
    "            i_, j_ = argmax_ij(word_distribution)\n",
    "            best_h_init[:, ii, :], best_c_init[:, ii, :] = h_init[:, i_, :], c_init[:, i_, :]\n",
    "            if sentence_batch[i_, i-1] == IC.word2idx['<eos>']:\n",
    "                best_word_idx[ii] = IC.word2idx['<eos>']\n",
    "                best_sentence[ii] = sentence_batch[i_]\n",
    "                best_sentence[ii][i] = IC.word2idx['<eos>']\n",
    "                likelihood_batch[ii] = word_distribution[i_, 0]\n",
    "                word_distribution[i_, :] = -np.inf\n",
    "            else:\n",
    "                best_word_idx[ii] = j_\n",
    "                best_sentence[ii] = sentence_batch[i_]\n",
    "                best_sentence[ii][i] = j_\n",
    "                likelihood_batch[ii] = word_distribution[i_, j_]\n",
    "                word_distribution[i_, j_] = -np.inf\n",
    "            if i==1:\n",
    "                word_distribution[:, j_]=-np.inf\n",
    "            else:\n",
    "                word_distribution[i_, j_] = -np.inf\n",
    "\n",
    "                    \n",
    "        h_init, c_init = best_h_init, best_c_init\n",
    "        h_init, c_init = IC.to_var(h_init), IC.to_var(c_init)\n",
    "        decoder_hidden_init = (h_init, c_init)\n",
    "        word_idx = best_word_idx\n",
    "        sentence_batch = best_sentence\n",
    "        \n",
    "        # print(\"word_idx (id)   : \",word_idx)\n",
    "        # print(\"word_idx (token): \",[IC.idx2word[x] for x in word_idx])\n",
    "        \n",
    "        # All batch is ended -- Terminate Condition\n",
    "        if (sentence_batch[:, i]==IC.word2idx['<eos>']).mean()==1: break\n",
    "            \n",
    "    sentence_batch = sentence_batch[:, 1:] # remove <sos> symbol\n",
    "    sentences = []\n",
    "    for i in range(K):\n",
    "        s = sentence_batch[i]\n",
    "        s = s[s!=IC.word2idx['<eos>']] # remove <eos> symbol\n",
    "        s = s[s!=IC.word2idx['<pad>']] # firstly initialize with sentence_batch = np.zeros()+<pad>\n",
    "        sentences.append(list(s))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_index:  14\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(12)\n",
    "# random.seed(12)\n",
    "sample_index = np.random.randint(0, IC.num_test+1)\n",
    "sample_index = 14\n",
    "print(\"sample_index: \",sample_index)\n",
    "(word, length, \n",
    "user, product, rating,\n",
    "memory_for, memory_word, memory_length) = IC.split_data(IC.test_data[sample_index:sample_index+1], sorting=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out ! love it !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out ! great writing !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down . ca n't wait for the next one to come out ! love it !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down , ca n't wait for the next one to come out ! great writing !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out ! very well worth it !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down . ca n't wait for the next one to come out ! great writing !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out ! great job !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down . ca n't wait for the next one to come out . very well worth reading .\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out ! would recommend it to everyone !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out ! very good writing !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out ! very well worth reading !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait to read the next one in the series ! great writing !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out ! very well worth the price !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down , ca n't wait for the next one to come out . very well worth reading .\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out . very well worth it !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out ! would recommend to everyone !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out ! very well worth the money !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out . very well worth the price !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out ! would recommend this book to everyone !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out ! very well worth your time !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down . ca n't wait for the next one to come out . very well worth your time .\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down . ca n't wait for the next one to come out . very well worth the money !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down . ca n't wait for the next one to come out . very well worth the price !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down . ca n't wait for the next one to come out . very well worth your time !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out ! very well worth the time !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down . ca n't wait for the next one to come out . very well worth the price .\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down , ca n't wait for the next one to come out . very well worth the money !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out . very well worth your time !\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down , ca n't wait for the next one to come out . very well worth your time .\n",
      "*****************************************************\n",
      "i loved this book , could n't put it down ! ca n't wait for the next one to come out ! great writing , great story line , great characters !\n",
      "*****************************************************\n"
     ]
    }
   ],
   "source": [
    "result = beam_search(model, IC=IC, id=sample_index, user=user[0], product=product[0], rating=rating[0], K=30)\n",
    "sentences = [\" \".join([IC.idx2word[x] for x in xs]) for xs in result]\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "    print(\"*****************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder_hidden' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-c84471edcc82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder_hidden' is not defined"
     ]
    }
   ],
   "source": [
    "decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'sure',\n",
       " 'what',\n",
       " 'to',\n",
       " 'expect',\n",
       " 'when',\n",
       " 'i',\n",
       " 'started',\n",
       " 'this',\n",
       " 'book',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'really',\n",
       " 'enjoyed',\n",
       " 'it',\n",
       " '.',\n",
       " 'it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'good',\n",
       " 'read',\n",
       " '.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[IC.idx2word[x] for x in result[2]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
